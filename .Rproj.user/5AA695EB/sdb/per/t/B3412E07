{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Animal movement with ![alt text](Images/Rlogo.jpeg)\"\nsubtitle: Animals in Motion 2017 Workshop - James Cook University \nauthor: Dr Ross Dwyer - The University of Queensland\ndate: May 16, 2017\noutput:\n  html_document:\n    toc: true\n    depth: 3\n    number_sections: true\n    theme: united\n    highlight: tango\n---\n\n--------------------------------------\n\n<br>  \n<br>  \n\n```{r, out.width = 1000, fig.retina = NULL, echo=FALSE}\nknitr::include_graphics(\"Images/Cassowary track.png\")\n```\n\n--------------------------------------\n\n#Introductions\n\n##Who am I?\nI am a spatial ecologist with a strong interest in understanding the drivers of movement, space use and community structure in free-ranging animals. I've used R for over 12 years, have published two R packages on CRAN and regularly teach R training courses at the University of Queensland and at other institutes around Australia.\n\nFor more information and teaching resources, check out [my website](https://www.uq.edu.au/eco-lab/content/dr-ross-dwyer).\n\n##Course outline\n\n**In this course you will learn how to analyse and interpret your telemetry datasets in R**. This workshop will hopefully show you how **R** can make the processing of spatial data much quicker and easier than using standard GIS software, and can help you plot some *deadly* figures! At the end of this workshop you will also have the annotated **R** code that you can re-run at any time, share with collaborators and build on with those newly acquired data!\n\nI designed this course not to comprehensively cover all the tools in **R**, but rather to teach you skills for taking your experience with R to the next level. Every new project comes with its own problems and questions and you will need to be independent, patient and creative to solve these challenges. It makes sense to invest time in becoming familiar with **R**, because today **R** is the leading platform for environmental data analysis and has some other functionalities which may surprise you! \n\nThis R workshop will be divided into 2 sessions intended to run about 1 hr 15 mins each.\n\n* Session 1: \n\n1.  **import** our raw data\n2.  calculate **distances travelled**\n3.  generate **home ranges**\n4.  **export** our spatial datasets\n\n* Session 2:\n5. **filter** our data using the [tidyverse](http://tidyverse.org/) group of R packages\n6.  generate plots using [ggplot2](http://ggplot2.org/)\n7.  generate interactive maps using [leaflet](http://leafletjs.com/) \n\n**The main principles I hope you will learn today are...**\n\n1. Data wrangling in **R** is safe, fast, reliable and repeatable\n2. R is the ideal suite for performing your GIS operations\n3. that in R, it is relativelty easy to produce amazing images and to share these with collaborators\n\n---------------------------------------\n\n##The Challenge\n\nThe process of turning raw telemetry data into publishable results is a highly involved process. Data sets are becoming larger, and larger as they are being gathered over longer time periods, over larger spatial extents and at increasing temporal resolutions. While this is increasing our ability to detect subtle patterns, these data sets are becoming so vast that they come with additional challenges.\n\nOur analysis may require us to filter these large data sets and integrate components with other data sets that we have gathered ourselves, acquired from our collaborators or downloaded from online repositories. These data may have different header rows, the values may be in the wrong format or likely cover different spatial or temporal scales. This spatial and temporal component may also be important - for integrating with other data sets or requiring us to apply specialised analytical techniques. Finally, multiple software platforms may be needed to process the data, run the analyses and generate results. \n\nThis process can require a huge investment in time: rearranging data, removing erroneous values, purchasing, downloading and learning the new software, and running analyses. Furthermore merging together excel spreadsheets, filtering data and preparing data for statistical analyses and plotting in different software packages can introduce all sorts of errors. \n\n*There is a better way...*\n\n---------------------------------------\n\n##Why use **R**\n\n**R** is a powerful language for data wrangling and analysis because...\n\n1. It is relatively **fast** to process commands\n2. You can create **repeatable** scripts\n3. You can **trace errors** back to their source\n4. You can **share your scripts** with other people\n5. It is **easy to identify errors** in large data sets\n6. Having your data in **R** opens up a huge array of **cutting edge analysis tools**.\n7. **R** is also totally **FREE!** \n\nAs **R** is open source, the more people we can get helping out on the **R** spatial mailing lists (e.g. **R-sig-geo**) and contributing their own spatial packages to the wider community, the more powerful **R** becomes!\n\n---------------------------------------\n\n##Obtaining **R**\n\nIf you do not currently have **R** installed on your machine, **R** can be downloaded from <http://cran.r-project.org/>\nUsing a script editor, such as **R Studio**, can make the writing and editing of code much more enjoyable and far less of a head-ache. \nIf you do not currently have **R Studio** installed on your machine, it can be downloaded from <http://www.rstudio.com/>. \n\nIt is worth updating these software on a regular basis as some of the most powerful new tools are only available on the latest release.\n\n---------------------------------------\n\n\n##Working with projects\n\nWe may want our R scripts to be saved into a place where they link seamlessly to other documents and data files for our research project. We may also want our tables, figures and statistical results to be written to locations on our computer where they are easy to locate.\n\nThe folder *JCU-2017-R-Workshop* that you have just downloaded from [github](https://github.com/RossDwyer/JCU-2017-R-Workshop) contains the course documents and the data we are going to be working with. \n\nFirst unzip this folder and extract the folder to a location on your computer that you typically store your research files. \n\nThe *JCU-2017-R-Workshop* folder is organised in the same format that I organise all my project files, and I have found it completely transferable across all the projects I am involved with. The main project folder contains the following sub folders:\n\n1.  Data\n2.  GIS\n3.  Images\n4.  R code\n\nThere is also a .Rproj file, which uses the `Project` functionality of R Studio to link your project within this working directory.\n\nWorking with Projects is a good idea because it saves the address that files are located and makes you coding much more elegant. It also allows you to share your script easily and seamlessly with collaborators.\n\nLastly there is a .Rmd file which constains the script I used to construct these course documents. Fancy huh?\n\n------------------------------------------\n\n![Southern cassowary with leg-mounted GPS tag](Images/caro_cass_tag.jpg)\n\n# Session 1: Introduction to spatial data \n\nIn this Session we are going to work with a data set containing locations from 3 GPS-tagged southern cassowaries (*Casuarius johnsonii*). These orphaned birds were tagged and released following up to 12 months rehabilitation at the [DERM facility at Garners Beach, N. QLD](http://savethecassowary.org.au/our-projects/cassowary-rehabilitation/). Their movements were tracked for between 6 and 20 weeks following the bird's release.\n\nThese data were obtained from the [ZoaTrack.org online telemetry database](https://zoatrack.org/projects/2), where they were publicly available under a Creative Commons Attribution License.  Terms of use require the following citations for both the data and the scientific manuscript.\n\n1.  Dwyer, R, Campbell, H (2016) Data from: 'Tracking juvenile cassowaries on Cape Tribulation using GPS-based telemetry'. ZoaTrack.org. doi: http://dx.doi.org/10.4226/68/5701F934048EE\n2.  Campbell, H.A., Dwyer, R.G., Sullivan, S., Mead, D., Lauridsen, G. (2014). Chemical immobilisation and satellite tagging of free-living southern cassowaries. Australian Veterinary Journal 92(7): 240-245. \n\n\n```{r, echo=FALSE,message=FALSE,include=TRUE}\n\nlibrary(leaflet)\nlibrary(htmlwidgets)\nlibrary(rgdal)\nlibrary(sp) \nlibrary(rgeos)\nlibrary(RColorBrewer)\nlibrary(knitr)\n\n## Import cleansed 2014 tagged animal file from ZoaTrack\ncassdat <- read.csv('Data/zoatrack-data.csv', header = TRUE)\ncassdat$DATE <- as.character(as.POSIXct(cassdat$DATE,format=\"%Y-%m-%d %H:%M:%S\"))\nsANIMALID <- levels(cassdat$ANIMALID)\n\npal <- colorFactor(c(\"red\",\"blue\",\"green\"), \n                   domain = as.character(sANIMALID))\ncassdat$ANIMALCOL <- ifelse(cassdat$ANIMALID==sANIMALID[1],\"red\",\n                            ifelse(cassdat$ANIMALID==sANIMALID[2],\"green\",\"blue\"))\n\ncassdat.sp <- cassdat\ncoordinates(cassdat.sp) <- ~LONGITUDE+LATITUDE\n\nleaflet(cassdat.sp) %>%\n  \n  #Base groups\n  addTiles(group = \"OSM (default)\") %>%  # Add default OpenStreetMap map tiles\n  addProviderTiles(\"Esri.WorldImagery\") %>%\n  \n  #Overlay groups\n  # All Data\n  addCircleMarkers(popup=paste(\"DATE=\",cassdat.sp$DATE,\"DELETED=\",cassdat.sp$DELETED,sep=(\" \")),\n                   weight=2,\n                   radius=5,\n                   color = cassdat.sp$ANIMALCOL,\n                   stroke = FALSE, fillOpacity = 0.4, group = \"Points\") %>%\n\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\",\"Esri.WorldImagery\"),\n    overlayGroups = c(\"Points\"),\n    options = layersControlOptions(collapsed = FALSE)) %>% \n  \n  #Legend\n  addLegend(\"topright\",\n            colors=c(\"red\",\"green\",\"blue\"), \n            labels=as.character(sANIMALID),\n            labFormat = labelFormat(prefix = \"$\"),\n            opacity = 1)\n\n```\nThe web map of cassowary GPS locations that we will make at the end of **Session 1**\n\n\n------------------------------------------\n\n## Reading telemetry data into **R**\n\nWe load a file into **R**, by telling it which file to load (`zoatrack_data.csv`) and where to find it (i.e. in the `Data` folder).\n\n```{r}\n# Load the cassowary data\ncassdat <- read.csv('Data/zoatrack-data.csv', header = TRUE)\n```\n\n\n**A note about Excel files**  \n\nDon’t use '.xlsx' or '.xls' files for saving data. The problem with '.xls' and '.xlsx' files are that they store extra info with the data that makes files larger than necessary and Excel formats can also unwittingly alter your data! \n\nA stable way to save your data is as a '.csv' file, which stands for 'comma separated values'. These are simply values separated by 'commas' and rows defined by 'returns'. If you select 'Save as’ in excel, you can choose '.csv' as one of the options. If you open the .csv file I have given you using a text editor, you will see it is just words, numbers and commas.\n\n##Accessing variables in the data frame\nThe rules we learned above for accessing values of an object also apply to data frames. A data frame is a special ‘class’ of an object, where there are multiple variables, stored in names columns and multiple rows for our samples. Every variable has the same number of samples. A key difference from the objects we created earlier is that now we have both rows and columns. Rows and columns are indexed by using a comma as a separator. Try the following code:\n\n```{r,eval=FALSE}\ncassdat[1,] # Provides the first row of data\ncassdat[1,3] # Provides the cell value from the 1st row and 3rd column\ncassdat[,3]  # Provides the first column of data\ncassdat[c(1,3),] # Provides the first and third date in the data frame\n```\n\nWe can also access objects in a data frame by their names:\n\n```{r,eval=FALSE}\ncassdat$DATE #provides all the data for that object\ncassdat$DATE[1:5] #provides athe first five values for that object\ncassdat[,'DELETED'] #provides all the data for that object\n```\n\nWe can get have a quick look at the data frame by typing:\n\n```{r,eval=FALSE}\nhead(cassdat) #first 6 rows by default\ntail(cassdat, 10) #last ten rows\n```\n\nThis functionality is particularly useful if the data frame is very large!\n\nNote the `()` around the data frame, as opposed to the `[]` we used for indexing. The `()` signify a *function*.\n\nWe can look at the data more closely using the `nrow`, `ncol`, `length`, `unique`, `str()` and `summary()` functions.\n\n```{r,eval=FALSE}\nnrow(cassdat) # number of rows in the data frame\nncol(cassdat) # number of columns in the data frame\n\nlength(cassdat$ANIMALID) # the number of values within a vector (same as the number of rows in the dataset)\n\nunique(cassdat$ANIMALID) # We use the unique function to extract the unique values for the tags deployed\n\nstr(cassdat) # provides internal structure of an R object\nsummary(cassdat) # Provides result summary of the data frame\n```\n\nThe `str()` function tell us how **R** has classified our objects into various data types. \n\nIn the result `summary()` of the dataframe, you can see what influence the *data type* has on the results that are presented. Those data columns that are are non-numeric and are not logicals (i.e. TRUE/FALSE) have been converted to factors (e.g. `ANIMALID` and `DATE`). For these factors, we are presented with a *frequency of occurences* for each factor level (note Annabelle has 1013 records). For those data columns that are numeric, we are presented with the minimum, mean, maximum and quartiles in the result summary.\n\n##Working with dates and times in **R**\n\nSee how **R** currently believes that our `DATE` column is a factorial? If we would like to plot our data in chronological order, or to extract the first or last locations for each animal, we first need to tell **R** that this `DATE` column represents actual dates.\n\nTo do this, we use the `as.POSIXct()` command, and state what datetime format our data is in using the `format = \"%\"` statement. Note that in this case we have the R default format of dates plus times: \"Y-m-d H:M:S\". \n\nWe then do some statistical summaries to make sure this is correct using the `min`, `max` and `summary` command in **R**.\n\n```{r,eval=FALSE}\ncassdat$DATE <- as.POSIXct(cassdat$DATE, format = \"%Y-%m-%d %H:%M:%S\")\n\nmin(cassdat$DATE) # First location in our dataset\nmax(cassdat$DATE) # Last location in our dataset\nsummary(cassdat$DATE) \n```\n\n##Filtering datasets\nThe `subset( )` function is one of the easiest way to select variables and observations. \n\nIn the following example, we subset our data for only those locations that have not been removed in ZoaTrack (`DELETED == FALSE`)\n\n```{r}\n# using subset function \ncassdat2 <- subset(cassdat, DELETED== \"FALSE\")\n```\n\nBy plotting these data, we can see which data points hace been removed\n\n```{r}\nplot(cassdat$LONGITUDE,cassdat$LATITUDE)\npoints(cassdat2$LONGITUDE,cassdat2$LATITUDE,col=cassdat$ANIMALID,pch=16)\n```\n\nFrom the interactive map at the beginning of this exercise, you will see that the deleted locations (white circles) were obatined prior to the devices being attached to the animals (i.e. at our accomodation in Cairns and enroute to our release site!)\n\nHowever, this plot is  a little misleading - the longitude and latitude fields have been distorted to fit on the x and y axes. In order to fix this, we'll need to install a few additional R packages.\n\n------------------------------------------\n\n##Packages\n\nPart of the reason R has become so popular is the vast array of packages that are freely available and highly accessible. In the last few years, the number of packages has grown exponentially - 8000 at my last check on CRAN! These can help you to do a galaxy of different things in R, including running complex analyses, drawing beautiful figures, running R as a GIS, constructing your own R packages, building web pages and even writing introductory R course handbooks!\n\nOnce we have installed all our required packages using the `install.packages()` function, we then load the required packages using the `library()` function.\n\n```{r, echo=FALSE}\nlibrary(sp)\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(adehabitatLT)\nlibrary(adehabitatHR)\n```\n\nHere we have installed and loaded our required **R** spatial packages, including the excellent adehabitatHR and adehabitatLS packages. These last two were built specifically for working with animal telemetry data.\n\n----------------------------\n\n![Cassowary tracks overlayed with the animal's home range](Images/caro_cass_dooley.jpg)\n##Working with spatial objects\n\nR offers a variety of functions for importing, manipulating, analyzing, and exporting spatial data. Although one might at first consider this to be the exclusive domain of GIS software, using R can frequently provide a much more lightweight, yet equally effective solution that embeds within a larger analytic workflow.\n\nOne of the tricky aspects of pulling spatial data into your analytic workflow is that there are numerous complicated data formats. In fact, even within R itself, functions from different user-contributed packages often require the data to be structured in very different ways. The good news is that efforts are underway to standardize spatial data classes in R. This movement is facilitated by sp, an important base package for spatial operations in R. It provides definitions for basic spatial classes (points, lines, polygons, pixels, and grids) in an attempt to unify the way R packages represent and manage these sorts of data. It also includes some core functions for creating and manipulating these data structures. The hope is that all spatial R packages will use (or at least provide conversions to) the 'Spatial' data class and it's derivatives, as now defined in the sp package. All else being equal, we favor R functions and packages that conform to the sp standard, as these are likely to provide the greatest future utility and durability.\n\n[Here](http://www.maths.lancs.ac.uk/~rowlings/Teaching/UseR2012/cheatsheet.html) is a very useful style giude for coding using `Spatial` objects. \n\n---------------------------\n\n##`SpatialPoints` and animal locations\n\nThe most basic spatial data object is a point, which may have 2 (X, Y) or 3 components (X, Y, Z). A single coordinate, or a collection of coordinates, may be used to define a `SpatialPoints` object.\n\nIn this exercise we are going to convert our cassowary locations from a standard data frame object into a `SpatialPointsDataFrame` object.\n\n```{r,echo=FALSE}\n# First make a duplicate of points.df to retain the non-spatial version\ncassdat.sp <- cassdat2\n\n# Now convert the data.frame object into a SpatialPoints object\ncoordinates(cassdat.sp)<- c(\"LONGITUDE\", \"LATITUDE\")\n\n## Have a look at the created object\nclass(cassdat.sp)\nstr(cassdat.sp)\n```\n\nNotice the class has now become a `SpatialPointsDataFrame`. The `str()` output contains lots of `@` symbols which denote a different slot in this S4 R object. Typing `points.sp@data` will extract the attribute data (similar to the attribute table in ArcGIS). The X and Y locational information can now be found in the `@coords` slot. In addition `@bbox` contains the bounding box coordinates and the `@pro4string` contains the projection, or coordinate reference system (CRS) information. \n\n```{r, eval=FALSE}\ncassdat.sp@coords\ncassdat.sp@proj4string\ncassdat.sp@bbox\nhead(cassdat.sp@data)\n\n# alternatively use the slot command to extract different \n# packages of data. As the data is stored in the data slot\nslot(cassdat.sp,'data') \n```\n\nNow let’s draw a simple spatial plot of the cassowary locations \n\n```{r}\n# First extract the names of cassowaries to help colour our plots\nmyids <- cassdat.sp@data[,1]\nunique(myids)\n\n# Now create a separate plot for each cassowary\npar(mfrow=c(1,3))\nplot(cassdat.sp[cassdat.sp$ANIMALID==\"Annabelle\",], col=\"red\",pch=16,main=\"Annabelle\")\nplot(cassdat.sp[cassdat.sp$ANIMALID==\"Dooley\",], col=\"green\",pch=16,main=\"Dooley\")\nplot(cassdat.sp[cassdat.sp$ANIMALID==\"M2\",], col=\"blue\",pch=16,main=\"M2\")\n```\n\n------------------------------------------\n\n##Coordinate Reference Systems (CRS)\n\nCentral to working with spatial data, is that these data have a coordinate reference system (CRS class) associated with it. Geographical CRS are expressed in degrees and associated with an ellipse, a prime meridian and a datum. Projected CRS are expressed in a measure of length and a chosen position on the earth, as well as the underlying ellipse, prime meridian and datum.\n\nMost countries have multiple coordinate reference systems, and where they meet there is usually a big mess — this led to the collection by the European Petroleum Survey Group (EPSG) of a geodetic parameter dataset.\n\nThe EPSG list among other sources is used in the workhorse PROJ.4 library, and handles transformation of spatial positions between different CRS. This library is interfaced with R in the rgdal package, and the CRS class is defined partly in the sp package and partly in rgdal.\n\nIn the next step, we need to define the CRS which corresponds to our dataset. For the cassowary GPS dataset (`cassdat.sp`), we have not specified the CRS so the `@pro4string` slot is empty at the moment (`= NA`). We therefore need to refer to the correct proj4 string information which is contained within the rgdal package.\n\nOur cassowary coordinates were collected in the **WGS 84** geographic datum in **Decimal Degrees**.\n\nFor simplicity, each projection can be referred to by a unique ID from the European Petroleum Survey Group (EPSG) geodetic parameter dataset. You can find the relevant EPSG code for your coordinate system from the website <http://spatialreference.org>. Here simply enter in a key word in the search box and select from the list the correct coordinate system. There is a map image in the top right of the site to help you. \n\nThe equivalent EPSG code for WGS 84 is [**4326**](http://spatialreference.org/ref/epsg/4326/)\n\nto set the spatial projection we use the `proj4string()` function\n\n```{r}\nWGS <- CRS(\"+init=epsg:4326\")\nproj4string(cassdat.sp) <- WGS\n```\n\nIn order to calculate distances and areas correctly, we need to now transform our data to the correct spatial projection. \n\n```{r, fig.retina = NULL, echo=FALSE}\nknitr::include_graphics(\"Images/AMGZones.jpg\")\n```\n\nFrom the above graphic, can you see which is the correct projection for the Cairns area - here's a [hint](http://spatialreference.org/ref/epsg/gda94-mga-zone-55/)\n\n```{r}\nGDA <- CRS(\"+init=epsg:28355\") #The equivalent EPSG code for WGS 84 is 28355. \ncassdat.P <- spTransform(cassdat.sp,GDA)\n```\n\n------------------------------------------\n\n##`SpatialPolygons` and home ranges\n\nIn the next section we will use functions contained in the [adehabitatHR](http://cran.r-project.org/web/packages/adehabitatHR/) R package to do some home range estimations. \n\nHome range estimators are often used in animal movement studies to identify important areas to wildlife\n\nMore information on these estimators (plus the full citations) can be found in the [R package vignette for adehabitatHR](http://127.0.0.1:12661/help/library/adehabitatHR/doc/adehabitatHR.pdf)\n\n\nThe function for calculating the kernel density is `kernelUD()`. In this exercise, we will first estimate the kernel utilization distribution (KUD), and plot this distribution on a map \n\n```{r}\n# Generate our Home Ranges\nkud <- kernelUD(cassdat.P[,1], h=\"href\") ## Estimation of UD for the three animals\nclass(kud)\nimage(kud) ## Plot the UD of the four animals\n```\n\nThe objects produced when running the `kernelUD()` function are 3 x `estUD` objects (one for each animal). Collectively adehabitat has classifyed them as an `estUDm` object. This adehabitat's own raster-style classification for a density surface.\n\nSuppose instead of a continuous surface we wanted to extract the 95% and 50% home range contours from the probability surface? We can do this using the `getverticeshr()` function.\n\n```{r}\n# Extract the 95% and 50% kerneal for each animal\n\n## 95% KUD\ncasskud.P95 <- getverticeshr(kud, percent = 95)\ncasskud.P95 ## Notice that this is now a SpatialPolygonsDataFrame object\n\n## 50% KUD\ncasskud.P50 <- getverticeshr(kud, percent = 50)\ncasskud.P50 ## Notice that this is now a SpatialPolygonsDataFrame object\n\n```\n\nThis generates a `SpatialPolygonDataframes` object, is a list of `Polygon` objects (such as animal home range contours) with CRS information and associated attribute data. \n\nWe can investigate the data contained within the object's data frame using the `@data` command. We can also plot these polygons individually by specifying what line of our `SpatialPolygonsDateFrame` to plot.\n\n\n```{r}\n# Home range size \ncasskud.P95@data\n\n# PLot the data for each animal as part of a 3 panel figure\npar(mfrow=c(1,3))\nplot(casskud.P95[1,],border=1,lty=2,main=\"Annabelle\")\nplot(casskud.P50[1,],col=\"lightgrey\", add=T)\nplot(cassdat.P[cassdat.P$ANIMALID==\"Annabelle\",], col=\"red\",pch=16, add=T)\n\nplot(casskud.P95[2,],border=1,lty=2,main=\"Dooley\")\nplot(casskud.P50[2,],col=\"lightgrey\", add=T)\nplot(cassdat.P[cassdat.P$ANIMALID==\"Dooley\",], col=\"green\",pch=16,add=T)\n\nplot(casskud.P95[3,],border=1,lty=2,main=\"M2\")\nplot(casskud.P50[3,],col=\"lightgrey\", add=T)\nplot(cassdat.P[cassdat.P$ANIMALID==\"M2\",], col=\"blue\",pch=16,add=T)\n\n```\n\n---------------------------\n\n##`SpatialLines` and animal trajectories\n\nIn this exercise we are going to extract movement metrics from along an animal's trajectory using the `as.ltraj` function in adehabitatLT. \n\n\n###Movement metrics\n\n`ltraj` objects were specifically designed to store the movements of animals monitored using tracking devices. \n\nOn running this function, the following parameters are automatically produced...\n\n1.  `dx, dy, dt`: these parameters measured at relocation i describe the increments of the x and y directions and time difference between  relocations i and i + 1.\n\n2.  `dist`: the distance between successive relocations \n\n3.  `abs.angle`: the absolute angle between the x direction and the step built by relocations i and i + 1.\n\n4.  `rel.angle`: the relative angle measures the change of direction between the step built by relocations i +􀀀 1 and i and the step built by relocations i and i + 1 (often called thwe \"turning angle\"). \n\n5.  `R2n`: the squared distance between the first relocation of the trajectory and the current relocation\n\n```{r}\n# Extract the date field, the animal id field and the coordinates of each relocation and save as a vector \nGPSdates <- as.POSIXct(cassdat.P@data$DATE)\nGPSids <- cassdat.P@data$ANIMALID\nGPSxys <- coordinates(cassdat.P)\n\n# Convert to an ltraj object\ncassdat.lt <- as.ltraj(xy=GPSxys,date=GPSdates,id=GPSids,proj4string = GDA) # Assign it the correct coordinate reference system\n\n#Have a look at the newly created ltraj object\ncassdat.lt\nhead(cassdat.lt[[1]])\n\n```\n\nYou'll see that the function `as.ltraj` has automatically computed the descriptive parameters from the x and y coordinates, and from the date. Note that `dx,dy,dist` are expressed in the units of the coordinates x,y (here, *metres*), the duration between relocations is in *seconds* and abs.angle,rel.angle are expressed in *radians*.\n\nA graphical display of the bursts can be obtained simply by:\n\n```{r}\nplot(cassdat.lt)\n```\n\n------------------------------------------\n\n###`SpatialLines` objects\n\nNext we will extract what is the minimum distance each cassowary travelled during the survey period. \n\nTo do this we are going to convert our data to a `SpatialLinesDataFrame`. `Line` objects are basically a collection of 2D coordinates linked together in a specific order by a series of lines. `Lines` objects are a list of Line objects, such as the contours on a single elevation, and each hold a specific ID (i.e. the depth of that particular contour). `SpatialLines` are formed by attributing the Lines object with spatial information related to the coordinate reference system. A `SpatialLinesDataFrame` is a spatial lines object with attribute data (e.g. animal id, distance travelled etc.)\n\nThe `ltraj2sldf` functions convert the class `ltraj` available in adehabitatLT towards  classes available in the package sp.\n\n```{r}\n# Convert the ltraj object into a spatial lines dataframe\ncasstrak.P <- ltraj2sldf(cassdat.lt,byid=TRUE)\n\n# Assign it the correct coordinate reference system\nproj4string(casstrak.P) <- GDA\n\nclass(casstrak.P)\n\n```\n\n------------------------------------------\n\n### Distance travelled\n\n`SpatialLines` and `SpatialLinesDataframes` can be very useful if you want to calculate the distance covered or the circumference of an area.\n\nHere we use the `SpatialLinesLengths()` function to calculate the minimum distance moved by each individual cassowary in meters\n\n```{r}\n# Extract the lengths of these lines in meters\n# 1: Annabelle; 2: Dooley; 3: M2\ncasstrak.P@data$Distance <- SpatialLinesLengths(casstrak.P)\n\ncasstrak.P@data\n```\n\n------------------------------------------\n\n##Writing a shapefile object from **R**\n\nOnce you have the `Spatial` object the way you like it, you will want to export it to view in a GIS. Here we show you two options for exporting your `Spatial` object, as a **shapefile** or as a **.kml** for viewing in [Google Earth](https://www.google.com/earth/). As with reading in spatial objects, there are a number of **R** packages out there to help you. Simply type `??kml` or `??shapefile` to look up a few of these options.\n\nIn this example we are goint to use the `writeOGR()` function in the rgdal package. to write the data containing the movement metrics to a shapefile.\n\nThe `ld` function allows to quickly convert an object of class ltraj to a data.frame. We then reproject our data back to Longs and Lats (WGS 1984) for viewing in Arc GIS or Google Earth\n\n```{r}\n\n# Convert the ltraj object into a standard data frame\ncassdat.lt.proj <- ld(cassdat.lt) # Change to a dataframe object\ncassdat.lt.proj$date <- as.POSIXct(cassdat.lt.proj$date) # Reassigns date as a date object\n\n# Convert the data frame object into a SpatialPointsDataFrame\ncoordinates(cassdat.lt.proj) <- ~x+y # Extract the coordinates\nproj4string(cassdat.lt.proj) <- GDA  # Assigns the coordinate reference system\n\n# Change it back to WGS 1984 projection\ncassdat.lt.WGS <- spTransform(cassdat.lt.proj,WGS)\n```\n\nFinally we run the `writeOGR` function to write this file to disk. We specify the folder as being within the GIS folder.\n\n```{r,eval=FALSE}\nwriteOGR(cassdat.lt.WGS, # This field needs to be a SpatialPoints*, SpatialLines* or SpatialPolygons* object\n         dsn=\"GIS\",\n         layer= \"cass_points\", driver=\"ESRI Shapefile\",\n         dataset_options=c(\"NameField=id\"))\n```\n\n\n*Extra task* - try writing the `SpatialPolygons` object containing our home ranges and our `SpatialLines` object containing our trajectory lines to file. \n\n------------------------------------------\n\n##Visualising your data in [Google Earth](https://www.google.com/earth/)\n\n###Visualising points\n\nGoogle Earth offers a highly flexible way of visualising your spatial data. `writeOGR()` in the rgdal library also offers this functionality. However the data **MUST** be geographical coordinates with datum WGS84.\n\nIf our data isn't already in the datum WGS84, We can transform it using the `spTransform()` function.\n\nThe [plotKML](http://plotkml.r-forge.r-project.org/) R package also offers some more advanced functionality for visualising your data in [Google Earth](https://www.google.com/earth/)). The file will be written in your project folder.\n\n```{r, eval=FALSE}\n## First lets export the cassowary locations (Points) (already in WGS84)\nwriteOGR(cassdat.lt.WGS,\n         dsn=\"GIS/cass_points.kml\",\n         layer= \"gps\", driver=\"KML\",\n         dataset_options=c(\"NameField=id\"))\n\n```\n\nIf you have time, try exporting your lines and polygons objects as .kml file. You will find help on how to do this by typing `?kml` or having a look on [Google](http://google.com)  \n\n------------------------------------------\n\n###Visualising animations\n\nNext convert your data into a STIDF object to create an animation of your Track\n\n```{r,eval=FALSE}\n# create a STIDF object:\nlibrary(spacetime)\nlibrary(plotKML)\nnewXY <- SpatialPoints(coordinates(cassdat.lt.WGS))\nproj4string(newXY) <- CRS(\"+proj=longlat +datum=WGS84\")\nnewXY.st <- STIDF(newXY, time = cassdat.lt.WGS@data$date, data = cassdat.lt.WGS@data[,c(\"id\")])\n \n# write to a KML file:\nshape <- \"http://maps.google.com/mapfiles/kml/pal2/icon18.png\"\nkml(newXY.st, dtime = 24*3600, colour = id, shape = shape, labels = \"\", kmz=TRUE)\n```\n\n***End of Session 1\n\n<br>  \n<br> \n------------------------------------------\n\n\n# Session 2: tidy style, pipes and ggplot2\n\n\n![The tidyverse](Images/TIDYVERSE.jpg)\n\n<br>  \n\nIn this session we will be having a look into the *tidyverse* group of R packages which have really revolutionised the way we work with big data and the way we visualise our results.\n\n##The tidyverse\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\n```\n\n###What is the tidyverse?\n* The [tidyverse](http://tidyverse.org/) is the collective name given to suite of R packages designed mostly by Hadley Wickham.\n\n* Before it was formalised in 2016 it was loosely referred to as the `hadleyverse`.\n* Packages share a common API and design philosophy intended to create a **\"Pit of Success\"**.\n\n###Members of the tidyverse\nbroom, **dplyr**, forcats, **ggplot2**, haven, httr, hms, jsonlite, lubridate, magrittr, modelr, purrr, **readr**, readxl, stringr, tibble, rvest, **tidyr**, xml2\n\n###tidy style\ntidyverse code has a certain look:\n```{r, eval = FALSE}\nexampledata %>%\n  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% \n  mutate(code = stringr::str_replace(code, \"newrel\", \"new_rel\")) %>%\n  separate(code, c(\"new\", \"var\", \"sexage\")) %>% \n  select(-new, -iso2, -iso3) %>% \n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n```\n\nggplot, one of the packages in the tidyverse, has a similar look...\n```{r, eval = FALSE}\nexampledata  %>% \nggplot(aes(x=ntags, y=nPU)) +\n  geom_point(alpha = 0.1,show_guide = F) +\n  geom_jitter(width = 0.01, height = 0.01) +\n  xlab(\"Number of tags\") +\n  ylab(\"Number of planning units\") +\n  theme_bw() +\n  theme(legend.title=element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank()) +\n  guides(color=guide_legend(\"season\"), fill = guide_legend(\"season\")) +\n  geom_smooth(method=\"lm\", formula = y~log(x), se = F, size = 1) \n```\n\n\n-[Grolemund, Wickham, 2016 R for Data Science](http://r4ds.had.co.nz/tidy-data.html#case-study)\n* The documentation of this style is still evolving.\n* [This is a good start](https://rpubs.com/yeedle/tidyguide)\n\n------------------------------------------\n\n##Loading in our spatial data\nFirst lets load the dataset we created in Session 1.\n\nIt is extremely easy to import spatial data into R. We will use the `readOGR` function from the rgdal package. This function is an interface to the OGR software, a famous GIS library used to import many different vector formats.\n\n```{r}\nlibrary(rgdal)\ncassdat.lt.WGS <- readOGR(\"GIS\", \"cass_points\")\n\n#check our data has loaded\nhead(cassdat.lt.WGS)\n```\n\n------------------------------------------\n\n##Introduction to pipes `%>%`\n\n![pipes are awesome!](Images/mario.png)\n\nNow that we've successfully loaded in our spatial dataset, lets start having a closer look at the data using pipes `%>%`\n\n* Originally from the `magrittr` package but has been imported to the `tidyverse`.\n* `%>%` is an **infix operator**. This means it takes two operands, left and right.\n* 'Pipes' the **output** of the last expression/function (left) forward to the **first input** of the next funciton (right).\n\n```{r, eval = FALSE}\n#For example, to see what class our data is in, we could use this code...\nclass(cassdat.lt.WGS)\n\n#Alternatively in the tidy verse we could use this code...\ncassdat.lt.WGS %>% class()\n```\n\n### Benefits of %>% and tidy style\n* Functions flow in natural order that tells story about data.\n* Code effects are easy to reason about by inserting `View()` or `head()` into pipe chain.\n* Common style makes it easy to understand collaborator (or your own) code.\n\n\n```{r}\n# First convert the spatial dataframe back to a data frame\ncassdat.df <- as.data.frame(cassdat.lt.WGS) \ncassdat.df$date <- as.POSIXct(cassdat.df$date)\n\n# Now insert functions into the pipe chain\n#cassdat.df %>% View()\ncassdat.df %>% head()\n```\n\n------------------------------------------\n\n## Dplyr\n* `dplyr` is the data wrangling workhorse of the tidyverse.\n* Provides functions, **verbs**, that can manipulate tibbles into the shape you need for analysis.\n* Has many backends allowing dplyr code to work on data stored in SQL databases and big data clusters.\n    - Works via translation to SQL. Keep an eye out for the SQL flavour in `dplyr`\n\n###Basic vocabulary\n* `select()` columns from a tibble \n* `filter()` to rows matching a certain condition\n* `mutate()` a tibble by changing or adding rows\n* `arrange()` rows in order\n* `group_by()` a variable\n* `summarise()` data over a group using a function\n\nCheck out this useful [Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) for data wrangling.\n\n\n###`select`\n\nWe can use the `select` function in Dplyr to choose the columns we want to include for our analyses and plotting\n\n```{r}\n# Select the rows we are interested in\ncassdat.df2 <- \n  cassdat.df %>% \n  select(coords.x1,coords.x2,id,date,dt,dist,abs_angle,rel_angle,R2n) %>%\n  select(-R2n)\n\nhead(cassdat.df2)\n```\n\n###`filter` and `arrange`\nSubset the data to rows matching logical conditions and then arrange according to particular attributes\n```{r, eval=FALSE}\nfilter(cassdat.df2, id == \"M2\") %>%\n  arrange(dist)\n\nfilter(cassdat.df2, id == \"M2\") %>%\n  arrange(desc(dist)) \n```\n\n###`mutate`\nAdd additional variables to an existing data frame\n```{r}\n# Add an hour onto the time fields\ncassdat.df3 <- \n  cassdat.df2 %>%\n  mutate(date2 = date + (60*60*10)) # Adds a date field and then changes the data from UTM to AEST)(i.e. adds 10 hrs)\n\n# Here we use functions contained inthe lubridate package to add date columns to our dataset\nlibrary(lubridate)\n\n# extract hour and month using lubridate function\n# attach the column using the mutate function\ncassdat.df4 <- \n  cassdat.df3 %>%\n  mutate(hourofday = hour(date2)) %>%\n  mutate(month = month(date2))\n\n## Now convert the duration between locations from seconds to minutes\ncassdat.df5 <-  cassdat.df4 %>%\n  mutate(dt_hr= (dt)/(60*60))\n```\n\n###`summarise`\nDetermine the total distances travelled for the total duration plus in a given month\n```{r}\ncassdat.df5 %>%\n  group_by(id) %>%\n  summarise(totaldist = sum(dist, na.rm=T))\n\ncassdat.df5 %>%\n  group_by(month, id) %>%\n  summarise(monthlydist = sum(dist, na.rm=T))\n```\n\n------------------------------------------\n\n## Data visualisation using `ggplot`\n\nggplot2 is a powerful data visualization package for the R programming language. The system provides **mappings** from your data to **aesthetics** which are used to construct beautiful plots.\n\nThe package makes it very easy to generate some very impressive figures and utilise a range of colour palettes. In fact it is so easy to use, most struggles with ggplot2 are really struggles with data!\n\nDocumentation for ggplot can be found [here](http://ggplot2.org/) and [here](http://docs.ggplot2.org/current/index.html).\n\nThere is also this awesome [Cheetsheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) for ggplot2\n\nIn the below script we call the data set we have just made (`cassdat.df5`) and then pipe it into the ggplot function. We than tell ggplot that we want to plot a box plot\n\n```{r}\nlibrary(ggplot2)   \n\ncassdat.df5 %>%\n  ggplot(mapping = aes(x = id, y = dt_hr)) + \n  geom_boxplot()\n\n```\n\nIn this case, we cant really see what's on the y axis as there's so many outliers. Lets put a log scale on the y axis to see it better and change the name of this axis to reflect our change\n\n```{r}\ncassdat.df5 %>%\n  ggplot(mapping = aes(x = id, y = dt_hr)) + \n  geom_boxplot() +\n  scale_y_log10() +\n  ylab(\"log(duration between locations)\")\n```\n\nWe can also use the `facet_wrap` function to generate a separate plot for each monthly category\n\n```{r}\ncassdat.df5 %>%\n  ggplot(mapping = aes(x = id, y = dt_hr, colour=id)) + \n  geom_boxplot() +\n  scale_y_log10() +\n  ylab(\"log(duration between locations)\") +\n  facet_wrap(~month)\n```\n\nNext we generate a circular plot to see when the locations for each bird were obtained\n\n```{r}\nlibrary(circular)\n\ncassdat.df5 %>%\nggplot(mapping = aes(x =  hourofday)) + \n  geom_histogram(breaks = seq(0, 24, by=1), colour = \"grey\") + \n  coord_polar(start = 0) + \n  facet_wrap(~id) +\n  theme_minimal()\n```\n\nNotice that the devices were programmed as to not obtain location data at night (i.e. when the birds were likely roosting). Notice how we used `theme_minimal()` to generate a simple plot without and extra colouring in the figure.\n\nIn the next exercise we'll generate a circular plot showing the direction  traveled by our tagged birds\n\n```{r}\nrad2deg <- function(rad) {(rad * 180) / (pi)}\n\ncassdat.df6 <- \n  cassdat.df5 %>%\n  mutate(abs_angle_deg = rad2deg(abs_angle)) %>%\n  mutate(rel_angle_deg = rad2deg(rel_angle))\n\ncassdat.df6 %>%\n  ggplot(mapping = aes(x =  rel_angle_deg)) + \n  geom_histogram(breaks = seq(-180, 180, by = 10), colour = \"grey\") + \n  coord_polar(start = 0) + theme_minimal()+\n  facet_wrap(~id) +\n  theme_minimal()\n```\n\n------------------------------------------\n\n##Interactive maps using leaflet\n\nFor our last task I want to show you how you can make some really cutting edge visuals with R, which will help you share your work with collaborators. We are going to make a simple web-based map using the  leaflet package. The leaflet package utilises the open-access leaflet JavaScript library (a web programming language), to create web-based maps. We will cover a basic map with some points on it today. See [Rstudio’s leaflet page](http://rstudio.github.io/leaflet/) for help and more mapping features, like polygons.\n\nTo get started with leaflet, first, make sure you have the leaflet and htmlwidgets packages installed, and then load it into this session:\n\n```{r}\nlibrary(leaflet)\nlibrary(htmlwidgets)\n```\n\nNext we'll add a new column to our data specifying the colour we want our points to be, before transforming our data to a `SpatialPointsDataframe` object.\n\n\n```{r}\n\n## Import cleansed 2014 tagged animal file from ZoaTrack\n\ncassdat.df7 <- cassdat.df6 %>%\n  mutate(ANIMALCOL = ifelse(id==\"Annabelle\",\"red\",\n                            ifelse(id==\"Dooley\",\"green\",\n                                   \"blue\")))\n\ncoordinates(cassdat.df7) <- ~coords.x1 + coords.x2\n\n```\n\n\nWe start by specifying a base layer, then we can add features to it. We will string together our layers using the pipes `(%>%)` command.\n\nThe below code first creates a map widget using the cassowary data frame. Then we `addTiles`, which is the background to the map. See [this page](http://rstudio.github.io/leaflet/basemaps.html) for the numerous base map options. Finally, we add some markers, located using the longitude and latitude variables in the cassowary data frame:\n\n```{r, eval = FALSE}\nmymap <- \nleaflet(cassdat.df7) %>%\n  \n  #Base maps\n  addTiles(group = \"OSM (default)\") %>%  # Add default OpenStreetMap map tiles\n  addProviderTiles(\"Esri.WorldImagery\") %>%\n\n  #Overlay points\n  addCircleMarkers(popup=paste0('id = ', cassdat.df7@data$id, \", DATE = \", cassdat.df7@data$date),\n                   weight=2,\n                   radius=5,\n                   color = cassdat.df7@data$ANIMALCOL,\n                   stroke = FALSE, fillOpacity = 0.4, group = \"Points\") %>%\n\n  #Add Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\",\"Esri.WorldImagery\"),\n    overlayGroups = c(\"Points\"),\n    options = layersControlOptions(collapsed = FALSE)) %>% \n  \n  #Add a Legend\n  addLegend(\"topright\",\n            colors=c(\"red\",\"green\",\"blue\"), \n            labels=c(\"Annabelle\", \"Dooley\",\"M2\"),\n            labFormat = labelFormat(prefix = \"$\"),\n            opacity = 1)\n\nmymap\n```\n\nFinally you can save this leaflet document at a .html file and share it with your friends or colleagues...\n\n```{r, eval = FALSE}\nsaveWidget(mymap, file=\"mymap.html\") # uses the htmldidgets package\n\n# Notice the file is saved to your working directory\n```\n\n*Extra task* - try writing the `SpatialPolygons` object containing our home ranges and our `SpatialLines` object containing our trajectory lines to this leaflet object \n\n***End of Session 2\n\n",
    "created" : 1487067123655.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2197214562",
    "id" : "B3412E07",
    "lastKnownWriteTime" : 1487068498,
    "last_content_update" : 1487068498014,
    "path" : "~/Documents/github files/JCU-2017-R-Workshop/workshop course materials Feb 2017.Rmd",
    "project_path" : "workshop course materials Feb 2017.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}